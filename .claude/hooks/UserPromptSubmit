#!/usr/bin/env python3
"""
UserPromptSubmit Hook - Autonomous Agent Handling System
Automatically detects task type, optimizes prompt, and routes to best agent
"""

import os
import sys
import json
from pathlib import Path
from datetime import datetime

# Read hook data from stdin
try:
    hook_data = json.load(sys.stdin)
except:
    sys.exit(0)

project_dir = os.environ.get('CLAUDE_PROJECT_DIR', '.')
CLAUDE_DIR = Path(project_dir) / ".claude"
AGENTS_DIR = CLAUDE_DIR / "agents"
TASKS_DIR = CLAUDE_DIR / "tasks"
SCOREBOARD_DIR = CLAUDE_DIR / "scoreboard"
PROMPTS_DIR = CLAUDE_DIR / "prompts"

# Get original user prompt
original_prompt = hook_data.get('userPrompt', '')

# Skip if no prompt or if already processed
if not original_prompt or '[Kiro-RLVR Agent Assignment]' in original_prompt:
    sys.exit(0)

def detect_task_type(prompt):
    """Automatically detect task type from user prompt"""
    prompt_lower = prompt.lower()
    
    # Keywords for each task type
    task_indicators = {
        'bugfix': ['fix', 'bug', 'error', 'issue', 'broken', 'crash', 'fail', 'debug', 'repair'],
        'feature': ['add', 'implement', 'create', 'new feature', 'build', 'develop', 'enhance'],
        'refactor': ['refactor', 'clean', 'improve', 'optimize', 'restructure', 'reorganize'],
        'security': ['security', 'vulnerability', 'exploit', 'injection', 'auth', 'permission', 'secure'],
        'testing': ['test', 'coverage', 'unit test', 'integration', 'testing', 'spec'],
        'performance': ['performance', 'speed', 'optimize', 'slow', 'efficiency', 'latency']
    }
    
    # Score each task type
    scores = {}
    for task_type, keywords in task_indicators.items():
        score = sum(1 for keyword in keywords if keyword in prompt_lower)
        if score > 0:
            scores[task_type] = score
    
    # Return highest scoring type, default to 'feature'
    if scores:
        return max(scores, key=scores.get)
    return 'feature'

def detect_priority(prompt):
    """Detect task priority from prompt"""
    prompt_lower = prompt.lower()
    
    if any(word in prompt_lower for word in ['urgent', 'critical', 'asap', 'immediately', 'emergency']):
        return 'P0'
    elif any(word in prompt_lower for word in ['important', 'priority', 'soon']):
        return 'P1'
    elif any(word in prompt_lower for word in ['minor', 'low priority', 'when possible']):
        return 'P3'
    return 'P2'

def optimize_prompt(prompt, task_type):
    """Optimize and enhance the user prompt"""
    
    # Load optimization templates
    optimization_templates = {
        'bugfix': """
Please provide the following details:
1. What is the expected behavior?
2. What is the actual behavior?
3. Steps to reproduce
4. Any error messages?
5. Affected files/components (if known)

Original request: {prompt}
""",
        'feature': """
Please consider:
1. User stories and use cases
2. Technical requirements
3. API design (if applicable)
4. UI/UX considerations
5. Testing approach
6. Documentation needs

Original request: {prompt}
""",
        'refactor': """
Please focus on:
1. Current code structure analysis
2. Identify code smells and issues
3. Propose improved architecture
4. Ensure backward compatibility
5. Update tests accordingly

Original request: {prompt}
""",
        'security': """
SECURITY TASK - Please prioritize:
1. Identify vulnerability type and severity
2. Assess potential impact
3. Implement secure fix
4. Add security tests
5. Update security documentation

Original request: {prompt}
""",
        'testing': """
Testing focus:
1. Identify what needs testing
2. Choose appropriate test types (unit/integration/e2e)
3. Aim for high coverage
4. Test edge cases
5. Ensure tests are maintainable

Original request: {prompt}
""",
        'performance': """
Performance optimization approach:
1. Profile and identify bottlenecks
2. Measure baseline performance
3. Implement optimizations
4. Verify improvements
5. Document changes

Original request: {prompt}
"""
    }
    
    template = optimization_templates.get(task_type, "Original request: {prompt}")
    return template.format(prompt=prompt)

def parse_simple_yaml(file_path):
    """Parse simple YAML without external dependencies"""
    data = {}
    current_key = None
    
    with open(file_path, 'r') as f:
        for line in f:
            line = line.rstrip()
            if not line or line.startswith('#'):
                continue
                
            if ':' in line and not line.startswith(' '):
                key, value = line.split(':', 1)
                key = key.strip()
                value = value.strip()
                data[key] = value
    
    return data

def load_agents():
    """Load available agents"""
    agents = []
    for agent_file in AGENTS_DIR.glob("agent-*.yml"):
        try:
            agent_data = parse_simple_yaml(agent_file)
            agent_data['name'] = agent_file.stem
            
            # Parse specializations
            if 'specializations' in agent_data:
                agent_data['specializations_list'] = [s.strip() for s in agent_data['specializations'].split('-')[1:]]
            
            if agent_data.get('tier') != 'suspended':
                agents.append(agent_data)
        except:
            continue
    return agents

def select_best_agent(task_type, priority, agents):
    """Select the best agent for the task"""
    tier_priority = {'principal': 3, 'senior': 2, 'junior': 1}
    best_agent = None
    best_score = -1
    
    for agent in agents:
        score = 0
        
        # Check specialization match
        specs = agent.get('specializations_list', [])
        if task_type in specs:
            score += 10
        elif 'general' in specs:
            score += 1
            
        # Add tier score
        tier = agent.get('tier', 'junior')
        score += tier_priority.get(tier, 1)
        
        # Boost principal agents for critical tasks
        if priority == 'P0' and tier == 'principal':
            score += 5
            
        # Consider performance
        perf_score = float(agent.get('rolling_avg_reward', '0') or '0')
        score += perf_score
        
        if score > best_score:
            best_score = score
            best_agent = agent
    
    return best_agent

# Detect task type and priority
task_type = detect_task_type(original_prompt)
priority = detect_priority(original_prompt)

# Load and select agent
agents = load_agents()
if not agents:
    # No agents available, proceed with original prompt
    sys.exit(0)

selected_agent = select_best_agent(task_type, priority, agents)
if not selected_agent:
    # Use first available agent
    selected_agent = agents[0]

# Create task ID
task_id = f"{task_type}-{datetime.now().strftime('%Y%m%d-%H%M%S')}"

# Optimize the prompt
optimized_prompt = optimize_prompt(original_prompt, task_type)

# Load agent's system prompt from file if available
agent_prompt = ""
agent_prompt_file = AGENTS_DIR / f"{selected_agent['name']}.yml"
if agent_prompt_file.exists():
    with open(agent_prompt_file, 'r') as f:
        in_prompt_section = False
        for line in f:
            if 'system_prompt:' in line:
                in_prompt_section = True
                continue
            elif in_prompt_section:
                if line.strip() and not line.startswith(' '):
                    break
                agent_prompt += line[2:] if line.startswith('  ') else line

# Create comprehensive agent context
agent_context = f"""[Kiro-RLVR Agent Assignment]

You are now operating as: {selected_agent['name']} ({selected_agent.get('tier', 'junior')} tier)
Task ID: {task_id}
Task Type: {task_type}
Priority: {priority}

=== AGENT PROFILE ===
{agent_prompt.strip()}

=== TASK ANALYSIS ===
Detected Type: {task_type}
Priority Level: {priority}
Optimization Applied: Yes

=== OPTIMIZED REQUEST ===
{optimized_prompt}

=== EVALUATION CRITERIA ===
Your performance will be evaluated on:
1. Test Coverage: Increase test coverage, never decrease it (30%)
2. Code Quality: Pass all lint checks, follow style guides (20%)
3. Security: No new vulnerabilities, fix existing ones (20%)
4. Complexity: Reduce complexity where possible (10%)
5. CI/CD: All pipelines must pass (10%)
6. Review: Clean, reviewable code with good practices (10%)

=== CONSTRAINTS ===
- You have access to these tools: {selected_agent.get('tools_allowed', 'all standard tools')}
- Maximum context tokens: {selected_agent.get('max_context_tokens', '75000')}
- Follow all project conventions and patterns
- Ensure backward compatibility unless explicitly asked to break it

=== APPROACH ===
Based on your tier ({selected_agent.get('tier', 'junior')}), you should:
""" + (
    "- Take initiative and make architectural decisions\n- Optimize for long-term maintainability\n- Set best practices for the team"
    if selected_agent.get('tier') == 'principal' else
    "- Implement robust solutions with good test coverage\n- Consider edge cases and error handling\n- Refactor code when it improves quality"
    if selected_agent.get('tier') == 'senior' else
    "- Follow instructions carefully\n- Ask for clarification when unsure\n- Focus on correctness over optimization"
) + f"""

Now, let's work on this {task_type} task with {priority} priority.
"""

# Save task metadata
metadata = {
    'task_id': task_id,
    'task_type': task_type,
    'priority': priority,
    'agent_name': selected_agent['name'],
    'agent_tier': selected_agent.get('tier', 'junior'),
    'timestamp': datetime.utcnow().isoformat(),
    'original_prompt': original_prompt,
    'optimized': True,
    'auto_detected': True
}

# Create task directory and save metadata
task_dir = TASKS_DIR / task_id
task_dir.mkdir(parents=True, exist_ok=True)
with open(task_dir / "metadata.json", 'w') as f:
    json.dump(metadata, f, indent=2)

# Log the assignment
log_entry = {
    'timestamp': datetime.utcnow().isoformat(),
    'event': 'autonomous_task_assigned',
    'task_id': task_id,
    'task_type': task_type,
    'priority': priority,
    'agent_name': selected_agent['name'],
    'agent_tier': selected_agent.get('tier', 'junior'),
    'prompt_optimized': True
}

log_file = SCOREBOARD_DIR / "events.jsonl"
log_file.parent.mkdir(parents=True, exist_ok=True)
with open(log_file, 'a') as f:
    f.write(json.dumps(log_entry) + '\n')

# Return the enhanced prompt
response = {
    "userPrompt": agent_context + "\n\n" + original_prompt
}

print(json.dumps(response))